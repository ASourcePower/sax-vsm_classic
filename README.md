SAX-VSM
============

This project provides the source code released to support our ICDM publication:

Senin, P.; Malinchik, S., [*SAX-VSM: Interpretable Time Series Classification Using SAX and Vector Space Model*](http://www2.hawaii.edu/~senin/assets/papers/sax-vsm-icdm13-short.FINAL_DRAFT.pdf) Data Mining (ICDM), 2013 IEEE 13th International Conference on, pp.1175,1180, 7-10 Dec. 2013.

that is based on the following work:

[1] Lin, J., Keogh, E., Wei, L. and Lonardi, S., [*Experiencing SAX: a Novel Symbolic Representation of Time Series*](http://cs.gmu.edu/~jessica/SAX_DAMI_preprint.pdf). [DMKD Journal](http://link.springer.com/article/10.1007%2Fs10618-007-0064-z), 2007.

[2] Salton, G., Wong, A., Yang., C., [*A vector space model for automatic indexing*](http://dl.acm.org/citation.cfm?id=361220). Commun. ACM 18, 11, 613â€“620, 1975.

[3] Finkel, Daniel E. [*DIRECT Optimization Algorithm User Guide*](http://www4.ncsu.edu/~ctk/Finkel_Direct/DirectUserGuide_pdf.pdf), 2003.

I am still working to make the code user friendly, if you have any suggestions, please email me.

1.0 BUILDING
------------
The code is written in Java and we use Ant to build it:
	
	$ ant -f jar.build.xml 
	Buildfile: /media/Stock/git/sax-vsm_classic.git/jar.build.xml
	...
	[jar] Building jar: /media/Stock/git/sax-vsm_classic.git/sax-vsm-classic20.jar
	[delete] Deleting directory /media/Stock/git/sax-vsm_classic.git/tmp
	BUILD SUCCESSFUL

2.0 FINDING THE BEST DISCRETIZATION PARAMETERS
------------
The code implements a modified for SAX-VSM DIRECT algorithm. Below is the trace of running sampler for Gun/Point dataset. The series in this dataset have length 150, so I define the sliding window range as [10-150], PAA size as [5-75] while the alphabet [2-18]. This is the run trace:

	$java -cp "sax-vsm-classic20.jar" edu.hawaii.jmotif.direct.SAXVSMDirectSampler data/Gun_Point/Gun_Point_TRAIN data/Gun_Point/Gun_Point_TEST 10 150 5 75 2 18 1 20
	running sampling for CLASSIC strategy...
	iteration: 0, minimal value 0.18 at 80, 40, 10
	iteration: 1, minimal value 0.04 at 80, 17, 10
	iteration: 2, minimal value 0.02 at 80, 17, 15
	min CV error 0.02 reached at [80, 17, 15], 
	running sampling for EXACT strategy...
	iteration: 0, minimal value 0.0 at 80, 40, 10
	iteration: 1, minimal value 0.0 at 80, 40, 10
	iteration: 2, minimal value 0.0 at 80, 40, 10
	min CV error 0.00 reached at [80, 40, 10], [33, 17, 10], 
	running sampling for NOREDUCTION strategy...
	iteration: 0, minimal value 0.0 at 80, 40, 10
	iteration: 1, minimal value 0.0 at 80, 40, 10
	iteration: 2, minimal value 0.0 at 80, 40, 10
	min CV error 0.00 reached at [80, 40, 10], [33, 17, 10], 
	classification results: CLASSIC, window 80, PAA 17, alphabet 15,  accuracy 0.94667,  error 0.05333
	classification results: EXACT, window 33, PAA 17, alphabet 10,  accuracy 0.98667,  error 0.01333
	classification results: NOREDUCTION, window 33, PAA 17, alphabet 10,  accuracy 0.98,  error 0.02

3.0 EXPLORING PATTERNS
------------
The class named `SAXVSMPatternExplorer` prints the most significant class-characteristic patterns, their weights, and the time-series that contain those. The `best_words_heat.R` script allows to plot these. Here is an example for the Gun/Point data:

![An example of class-characteristic patterns locations in Gun/Point data](https://raw.githubusercontent.com/jMotif/sax-vsm_classic/master/RCode/figures/gun_point_heat.png)

4.0 CLASSIFICATION
------------
`SAXVSMClassifier` implements the classification procedure. It reads both tran and test datasets, discretizes series, builds TFIDF vectors, and performs the classifciation:

	$ java -cp "sax-vsm-classic20.jar" edu.hawaii.jmotif.direct.SAXVSMClassifier data/Gun_Point/Gun_Point_TRAIN data/Gun_Point/Gun_Point_TEST 33 17 5 EXACT
	processing paramleters: [data/Gun_Point/Gun_Point_TRAIN, data/Gun_Point/Gun_Point_TEST, 33, 17, 5, EXACT]
	...
	classification results: EXACT, window 33, PAA 17, alphabet 5,  accuracy 0.98,  error 0.02
	
5.0 NOTES
------------
Note, that by default, for the best parameters validation on TEST data, the sampling routine chooses a parameters set corresponding to the shortest sliding window, which you may want to change - for example to choose the point which neighborhood contains the most sampled density.

Also note that code implements 5 ways the TF (term frequency value) can be computed:

	double tfValue = Math.log(1.0D + Integer.valueOf(wordInBagFrequency).doubleValue());
	// double tfValue = 1.0D + Math.log(Integer.valueOf(wordInBagFrequency).doubleValue());
	// double tfValue = normalizedTF(bag, word.getKey());
	// double tfValue = augmentedTF(bag, word.getKey());
	// double tfValue = logAveTF(bag, word.getKey());

For many datasets, these yield quite different accuracy.

Finally, note, that when cosine similarity is computed within the classification procedure, it may happen that its value is the same for all classes. In a such case the current classifier' implementation considers that the time series was missclassified, but you may want to assign it to one of the classes randomly.

